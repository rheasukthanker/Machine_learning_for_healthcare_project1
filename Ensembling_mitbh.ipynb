{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train = pd.read_csv(\"mitbih_train.csv\", header=None)\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_test = pd.read_csv(\"mitbih_test.csv\", header=None)\n",
    "\n",
    "Y = np.array(df_train[187].values).astype(np.int8)\n",
    "X = np.array(df_train[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "Y_test = np.array(df_test[187].values).astype(np.int8)\n",
    "X_test = np.array(df_test[list(range(187))].values)[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_OUTPUT_UNITS = [64, 128]\n",
    "import keras\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, \\\n",
    "    concatenate\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "def get_model():\n",
    "    inp = Input(shape=(187, 1))\n",
    "    current_layer = keras.layers.Masking(mask_value=0., input_shape=(187, 1), name='Masked')(inp)\n",
    "    for i, size in enumerate(RNN_OUTPUT_UNITS):\n",
    "        notLast = i + 1 < len(RNN_OUTPUT_UNITS)\n",
    "        layer = keras.layers.LSTM(size, return_sequences=notLast, dropout=0.2, name = 'LSTM' + str(i+1))\n",
    "        current_layer = keras.layers.Bidirectional(layer, name = 'BiRNN' + str(i+1))(current_layer)\n",
    "    current_layer = keras.layers.Dense(128, name='Dense1', activation='relu')(current_layer)\n",
    "    current_layer = keras.layers.Dense(64, name='Dense2', activation='relu')(current_layer)\n",
    "    current_layer = keras.layers.Dense(16, name='Dense3', activation='relu')(current_layer)\n",
    "    logits = keras.layers.Dense(5, name='Output', activation='softmax')(current_layer)\n",
    "    model = models.Model(inputs=inp, outputs=logits)\n",
    "    opt = optimizers.Adam(0.001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 187, 1)            0         \n",
      "_________________________________________________________________\n",
      "Masked (Masking)             (None, 187, 1)            0         \n",
      "_________________________________________________________________\n",
      "BiRNN1 (Bidirectional)       (None, 187, 128)          33792     \n",
      "_________________________________________________________________\n",
      "BiRNN2 (Bidirectional)       (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "Dense3 (Dense)               (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 339,237\n",
      "Trainable params: 339,237\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model=get_model()\n",
    "model.load_weights(\"baseline_bidir_lstm_mitbih.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_bidir_logits= model.predict(X_test)\n",
    "pred_test_bidir= np.argmax(pred_test_bidir_logits, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(187, 1))\n",
    "    current_layer = keras.layers.Masking(mask_value=0., input_shape=(187, 1), name='Masked')(inp)\n",
    "    for i, size in enumerate(RNN_OUTPUT_UNITS):\n",
    "        notLast = i + 1 < len(RNN_OUTPUT_UNITS)\n",
    "        current_layer = keras.layers.SimpleRNN(size, return_sequences=notLast, dropout=0.2, name = 'rnn' + str(i+1))(current_layer)\n",
    "\n",
    "    current_layer = keras.layers.Dense(64, name='Dense1', activation='relu')(current_layer)\n",
    "    current_layer = keras.layers.Dense(16, name='Dense2', activation='relu')(current_layer)\n",
    "    logits = keras.layers.Dense(5, name='Output', activation='softmax')(current_layer)\n",
    "    model = models.Model(inputs=inp, outputs=logits)\n",
    "    opt = optimizers.Adam(0.001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 187, 1)            0         \n",
      "_________________________________________________________________\n",
      "Masked (Masking)             (None, 187, 1)            0         \n",
      "_________________________________________________________________\n",
      "rnn1 (SimpleRNN)             (None, 187, 64)           4224      \n",
      "_________________________________________________________________\n",
      "rnn2 (SimpleRNN)             (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 38,309\n",
      "Trainable params: 38,309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=get_model()\n",
    "model.load_weights(\"rnn_simple_mitbih.h5\")\n",
    "pred_test_rnn_logits = model.predict(X_test)\n",
    "pred_test_rnn= np.argmax(pred_test_rnn_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 187, 1)            0         \n",
      "_________________________________________________________________\n",
      "Masked (Masking)             (None, 187, 1)            0         \n",
      "_________________________________________________________________\n",
      "lstm1 (LSTM)                 (None, 187, 64)           16896     \n",
      "_________________________________________________________________\n",
      "lstm2 (LSTM)                 (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 125,093\n",
      "Trainable params: 125,093\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(187, 1))\n",
    "    current_layer = keras.layers.Masking(mask_value=0., input_shape=(187, 1), name='Masked')(inp)\n",
    "    for i, size in enumerate(RNN_OUTPUT_UNITS):\n",
    "        notLast = i + 1 < len(RNN_OUTPUT_UNITS)\n",
    "        current_layer = keras.layers.LSTM(size, return_sequences=notLast, dropout=0.2, name = 'lstm' + str(i+1))(current_layer)\n",
    "\n",
    "    current_layer = keras.layers.Dense(64, name='Dense1', activation='relu')(current_layer)\n",
    "    current_layer = keras.layers.Dense(16, name='Dense2', activation='relu')(current_layer)\n",
    "    logits = keras.layers.Dense(5, name='Output', activation='softmax')(current_layer)\n",
    "    model = models.Model(inputs=inp, outputs=logits)\n",
    "    opt = optimizers.Adam(0.001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "model=get_model()\n",
    "model.load_weights(\"lstm_simple_mitbih.h5\")\n",
    "pred_test_lstm_logits= model.predict(X_test)\n",
    "pred_test_lstm= np.argmax(pred_test_lstm_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 187, 1)            0         \n",
      "_________________________________________________________________\n",
      "Masked (Masking)             (None, 187, 1)            0         \n",
      "_________________________________________________________________\n",
      "GRU1 (GRU)                   (None, 187, 64)           12672     \n",
      "_________________________________________________________________\n",
      "GRU2 (GRU)                   (None, 128)               74112     \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 16)                1040      \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 96,165\n",
      "Trainable params: 96,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(187, 1))\n",
    "    current_layer = keras.layers.Masking(mask_value=0., input_shape=(187, 1), name='Masked')(inp)\n",
    "    for i, size in enumerate(RNN_OUTPUT_UNITS):\n",
    "        notLast = i + 1 < len(RNN_OUTPUT_UNITS)\n",
    "        current_layer = keras.layers.GRU(size, return_sequences=notLast, dropout=0.2, name = 'GRU' + str(i+1))(current_layer)\n",
    "\n",
    "    current_layer = keras.layers.Dense(64, name='Dense1', activation='relu')(current_layer)\n",
    "    current_layer = keras.layers.Dense(16, name='Dense2', activation='relu')(current_layer)\n",
    "    logits = keras.layers.Dense(5, name='Output', activation='softmax')(current_layer)\n",
    "    model = models.Model(inputs=inp, outputs=logits)\n",
    "    opt = optimizers.Adam(0.001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "model=get_model()\n",
    "file_path = \"gru_simple_mitbih.h5\"\n",
    "model.load_weights(file_path)\n",
    "pred_test_gru_logits= model.predict(X_test)\n",
    "pred_test_gru= np.argmax(pred_test_gru_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           (None, 187, 1)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 183, 32)      192         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_1 (Conv1D)                (None, 183, 32)      5152        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Conv2_1 (Conv1D)                (None, 183, 32)      5152        Conv1_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ResidualSum_1 (Add)             (None, 183, 32)      0           Conv2_1[0][0]                    \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Act_1 (Activation)              (None, 183, 32)      0           ResidualSum_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "MaxPool_1 (MaxPooling1D)        (None, 90, 32)       0           Act_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_2 (Conv1D)                (None, 90, 32)       5152        MaxPool_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Conv2_2 (Conv1D)                (None, 90, 32)       5152        Conv1_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ResidualSum_2 (Add)             (None, 90, 32)       0           Conv2_2[0][0]                    \n",
      "                                                                 MaxPool_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Act_2 (Activation)              (None, 90, 32)       0           ResidualSum_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "MaxPool_2 (MaxPooling1D)        (None, 43, 32)       0           Act_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_3 (Conv1D)                (None, 43, 32)       5152        MaxPool_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Conv2_3 (Conv1D)                (None, 43, 32)       5152        Conv1_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ResidualSum_3 (Add)             (None, 43, 32)       0           Conv2_3[0][0]                    \n",
      "                                                                 MaxPool_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Act_3 (Activation)              (None, 43, 32)       0           ResidualSum_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "MaxPool_3 (MaxPooling1D)        (None, 20, 32)       0           Act_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_4 (Conv1D)                (None, 20, 32)       5152        MaxPool_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Conv2_4 (Conv1D)                (None, 20, 32)       5152        Conv1_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ResidualSum_4 (Add)             (None, 20, 32)       0           Conv2_4[0][0]                    \n",
      "                                                                 MaxPool_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Act_4 (Activation)              (None, 20, 32)       0           ResidualSum_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "MaxPool_4 (MaxPooling1D)        (None, 8, 32)        0           Act_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_5 (Conv1D)                (None, 8, 32)        5152        MaxPool_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Conv2_5 (Conv1D)                (None, 8, 32)        5152        Conv1_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ResidualSum_5 (Add)             (None, 8, 32)        0           Conv2_5[0][0]                    \n",
      "                                                                 MaxPool_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Act_5 (Activation)              (None, 8, 32)        0           ResidualSum_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "MaxPool_5 (MaxPooling1D)        (None, 2, 32)        0           Act_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 64)           0           MaxPool_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FC1 (Dense)                     (None, 128)          8320        flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FC2 (Dense)                     (None, 32)           4128        FC1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "Output (Dense)                  (None, 5)            165         FC2[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 64,325\n",
      "Trainable params: 64,325\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, \\\n",
    "    concatenate\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "def conv_unit(unit, input_layer):\n",
    "    s = '_' + str(unit)\n",
    "    layer = Convolution1D(name='Conv1' + s, filters=32, kernel_size=5, strides=1, padding='same', activation='relu')(input_layer)\n",
    "    layer = Convolution1D(name='Conv2' + s, filters=32, kernel_size=5, strides=1, padding='same', activation=None)(layer )\n",
    "    layer = keras.layers.Add(name='ResidualSum' + s)([layer, input_layer])\n",
    "    layer = keras.layers.Activation(\"relu\", name='Act' + s)(layer)\n",
    "    layer = keras.layers.MaxPooling1D(name='MaxPool' + s, pool_size=5, strides=2)(layer)\n",
    "    return layer\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(187, 1))\n",
    "    current_layer = Convolution1D(filters=32, kernel_size=5, strides=1)(inp)\n",
    "\n",
    "    for i in range(5):\n",
    "        current_layer = conv_unit(i + 1, current_layer)\n",
    "\n",
    "    current_layer = keras.layers.Flatten()(current_layer)\n",
    "    current_layer = keras.layers.Dense(128, name='FC1', activation='relu')(current_layer)\n",
    "    current_layer = keras.layers.Dense(32, name='FC2', activation='relu')(current_layer)\n",
    "    logits = keras.layers.Dense(5,activation=activations.softmax, name='Output')(current_layer)\n",
    "\n",
    "    model = models.Model(inputs=inp, outputs=logits)\n",
    "    opt = optimizers.Adam(0.001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=losses.sparse_categorical_crossentropy, metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model\n",
    "model = get_model()\n",
    "file_path = \"residual_cnn_mitbih.h5\"\n",
    "model.load_weights(file_path)\n",
    "pred_test_res_cnn_logits= model.predict(X_test)\n",
    "pred_test_res_cnn= np.argmax(pred_test_res_cnn_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#majority voting\n",
    "all_votes=np.concatenate([np.reshape(pred_test_bidir,[21892,1]),np.reshape(pred_test_res_cnn,[21892,1]),np.reshape(pred_test_gru,[21892,1]),np.reshape(pred_test_lstm,[21892,1]),np.reshape(pred_test_rnn,[21892,1])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [4, 4, 4, 0, 0],\n",
       "       [4, 4, 4, 4, 0],\n",
       "       [4, 4, 4, 4, 0]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#majority vote based ensembling\n",
    "from scipy.stats import mode\n",
    "modes=mode(all_votes,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes_only=np.array(np.squeeze(modes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modes_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score : 0.785320999000833 \n",
      "Test accuracy score : 0.9572903343687191 \n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(Y_test, modes_only,average=\"macro\")\n",
    "\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "acc = accuracy_score(Y_test, modes_only)\n",
    "\n",
    "print(\"Test accuracy score : %s \"% acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression based ensembling\n",
    "true_logits_bidir=[]\n",
    "for i in range(0,np.shape(pred_test_bidir)[0]):\n",
    "    true_logits_bidir.append(pred_test_bidir_logits[i,Y_test[i]])\n",
    "true_logits_bidir=np.array(true_logits_bidir)\n",
    "true_logits_lstm=[]\n",
    "for i in range(0,np.shape(pred_test_lstm)[0]):\n",
    "    true_logits_lstm.append(pred_test_lstm_logits[i,Y_test[i]])\n",
    "true_logits_lstm=np.array(true_logits_lstm)\n",
    "true_logits_gru=[]\n",
    "for i in range(0,np.shape(pred_test_gru)[0]):\n",
    "    true_logits_gru.append(pred_test_gru_logits[i,Y_test[i]])\n",
    "true_logits_gru=np.array(true_logits_gru)\n",
    "true_logits_rnn=[]\n",
    "for i in range(0,np.shape(pred_test_rnn)[0]):\n",
    "    true_logits_rnn.append(pred_test_rnn_logits[i,Y_test[i]])\n",
    "true_logits_rnn=np.array(true_logits_rnn)\n",
    "true_logits_res_cnn=[]\n",
    "for i in range(0,np.shape(pred_test_res_cnn)[0]):\n",
    "    true_logits_res_cnn.append(pred_test_res_cnn_logits[i,Y_test[i]])\n",
    "true_logits_res_cnn=np.array(true_logits_res_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_logits=np.concatenate([np.reshape(true_logits_bidir,[21892,1]),\n",
    "                           np.reshape(true_logits_rnn,[21892,1]),\n",
    "                           np.reshape(true_logits_lstm,[21892,1]),\n",
    "                           np.reshape(true_logits_gru,[21892,1]),\n",
    "                           np.reshape(true_logits_res_cnn,[21892,1])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_logits_sum=np.sum(full_logits,axis=1)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rheasukthanker/opt/anaconda3/envs/renv2/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(full_logits,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=clf.predict(full_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 4, 4], dtype=int8)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1 score : 0.6012732618733752 \n",
      "Test accuracy score : 0.9413484377854924 \n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(Y_test, preds,average=\"macro\")\n",
    "\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "acc = accuracy_score(Y_test, preds)\n",
    "\n",
    "print(\"Test accuracy score : %s \"% acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
